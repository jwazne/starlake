# Load

Load and validate, in one shot or incrementally, JSON, XML and CSV files into your datawarehouse using different write strategies.



## Prerequisites
This step assumes we loaded we have the data located in the incoming folder as described in the [Extract](../extract/README.md) step.

If you skipped the extract step above, you can download the data from [here](https://github.com/starlake-ai/starlake/tree/master/samples/starbake/incoming/starbake).
and copy them to the incoming/starbake folder.
The folder contains now three files:
- product.xml
- order_20240228.json
- order_line_20240228.csv

## Infer schema
This is done by running the following command:

```bash
starlake infer-schema --input-path incoming/starbake --clean
```

Your metadata `load` folder should now contain the folder `starbake` with following files:
- products.sl.yml
- orders.sl.yml
- order_lines.yml
- _config.sl.yml

The Ã¬nfer-schema` command has created a schema file for each of the files in the incoming folder trying to detect the schema of the files.
These schema files are used to load the data into the datawarehouse.

In a real life scenario, you may want to review the schema files and adjust them to your needs.

## Stage before loading
starlake can stage the files before loading them into the datawarehouse.
This is useful when your files arrive in a different folder from the one where they are loaded into the datawarehouse.

To move the incoming files to the stage folder. Run the following command:

```bash
starlake stage
```


## Load data into your datawarehouse

Run the following command to load the files in the incoming folder.
Since we are target teh DuckDB datawarehouse, we need to set the SL_ENV variable to `DUCKDB` to activate the env.DUCKDB.sl.yml configuration file.

```bash

SL_ENV=DUCKDB starlake stageload

```

## Apply transformations on the fly

## Customize write strategies

## Partition your tables


