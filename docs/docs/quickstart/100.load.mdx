---
sidebar_position: 100
title: Load
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

In this section you will learn how to load and transform data using the samples files created when [bootstrapping a new project](bootstrap).
You will learn how to:
- import: recognize new arriving files that need to be loaded into your warehouse
- load: validate file records and load data into you warehouse
- transform: apply transformation on your previously loaded data


## Sample scenario

Say we have to load the `customers` file into the warehouse.
The customers are provided by the "sales" department as delimited separated values files
and are required to be loaded incrementally.


#### customers dataset

``File customers-2018-01-01.psv from "sales" department``

|id|signup|contact|birthdate|firstname|lastname|country|
|---|---|---|---|---|---|---|
|A009701|2010-01-31 23:04:15|k@m.com|1980-04-15|Kylian|Mbapp√©|France|
|B000001|2016-12-01 09:56:02|n@b.com|1980-04-15|Napoleon|Bonaparte|France|
|B000001|2016-12-02 09:56:02|m@c.com|1980-04-15|Marie|Curie|France|
|B000002|2016-12-02 09:56:02|z@z.com|1980-04-15|Zinedine|Zidane|France|
|B000003|2016-12-03 09:56:02|e@g.com|1980-04-15|Eva|Green|France|
|B000012|2016-12-03 09:56:02|k@b.com|1980-04-15|Karim|Benzema|France|
|B000004|2016-12-04 09:56:02|m@c.com|1980-04-15|Marion|Cotillard|France|
|B000005|2016-12-05 09:56:02|a@g.com|1980-04-15|Ariana|Grande|USA|
|B000006|2016-12-06 09:56:02|m@j.com|1980-04-15|Michael|Jordan|USA|
|B000007|2016-12-07 09:56:02|m@a.com|1980-04-15|Muhammad|Ali|USA|
|B000008|2016-12-08 09:56:02|t@s.com|1980-04-15|Taylor|Swift|USA|
|B000009|2016-12-09 09:56:02|e@p.com|1980-04-15|Elvis|Presley|USA|
|B000010|2016-12-10 09:56:02|s@j.com|1980-04-15|Steve|Jobs|USA|
|B000011|2016-12-11 09:56:02|a@l.com|1980-04-15|Abraham|Lincoln|USA|


## Infer the schema

In Starlake terms, after loading, we will end up with:
- one domain: `sales` . A domain is equivalent to a database schema or a BigQuery dataset.
- one table: the `customers` table in the `sales` domain 

We first need to write a YAML configuration file that describe the structure of the file to load into the warehouse.
Instead of writing this file by hand, we may infer this YAML configuration file using the `infer-schema` command.



<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh infer-schema               \
    --domain sales                                      \
    --table customers                                   \
    --input incoming/customers-2018-01-01.psv     \
    --output-dir metadata/load --with-header
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd infer-schema  ^
    --domain sales                                                      ^
    --table customers                                                   ^
    --input incoming/customers-2018-01-01.psv                     ^
    --output-dir metadata/load --with-header
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e SL_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake infer-schema     \
    --domain sales                                                  \
    --table customers                                               \
    --input $HOME/myproject/incoming/customers-2018-01-01.psv \
    --output-dir $HOME/myproject/metadata/load                   \
    --with-header```
```

</TabItem>
</Tabs>

This command will infer the YML configuration file used by the loader to watch the folder for new files to load and to validate each record in those files.
This file may be found in the `customers.comet.yml` file under the `metadata/load/sales` folder.
Notice how the `customers.comet.yml` file is named after the _table_ name and stored in the folder named after the _domain_ name.
The domain configuration file _config.comet.yml_ is stored in the _domain_ folder.


The contents of the files look like this:

### Domain configuration file: _config.comet.yml
This file describes properties shared by all tables in this domain.
Here we assume that all tables in the `sales` domain are loaded from the _incoming_ folder.
`{{root_path}}` is a variable path set in the env file (more on this later).

```yaml title="Domain configuration file: _config.comet.yml"
---
load:
  name: "sales"
  metadata:
    directory: "{{root_path}}/incoming"
```


### Table configuration file: customers.comet.yml

The YAML file describes the structure of the file to load into the warehouse.
The `pattern` property is a regular expression that will be used to match the file name.

```yaml title="Table schema: customers.comet.yml"
---
table:
  name: "customers"   # destination table name
  pattern: "customers-2018-01-01.psv" # This property is a regular expression that will be used to match the file name.
                                      # Please replace it by a file pattern eq. customers-.*.psv
  attributes:         # Description of the fields to recognize
    - name: "id"        # attribute name and column name in the destination table if no rename attribute is defined
      type: "string"    # expected type
      array: false      # is it an array (false by default, ignored in DSV files) ?
      required: false   # Is this field required in the source (false by default, change it accordingly) ?
      privacy: "NONE"   # Should we crypt this field before loading to the warehouse (No crypting by default )?
      ignore: false     # Should this field be excluded (false by default) ?
    - name: "signup"    # second attribute
      type: "timestamp" # recognized type by analyzing input.
    - name: "contact"
      type: "string"
      ...
    - name: "birthdate"
      type: "date"      # recognized as semantic type date.
      ...
    - name: "firstname"
      type: "string"
      ...
    - name: "lastname"
      type: "string"
      ...
    - name: "country"
      type: "string"
      ...               # and so on ...
  metadata:
    mode: "FILE"
    format: "DSV"
    encoding: "UTF-8"
    multiline: false
    array: false
    withHeader: true
    separator: "|"
    quote: "\""
    escape: "\\"

```

We have seen in the [bootstrap section](bootstrap) that three environment files were created such as `metadata/env.comet.yml`.
These environment files define variables that will be used for runtime substitution in any YAML file. Variables in YAML files are enclosed with {{ and }}.

Furthermore, it is important to know that `metadata/env.comet.yml` define the default variables.
Any other environment files will be mixed with this default one and specific environment file have higher precedence.
In other word, we have the following environments

| Environment | File used                          |
|-------------|------------------------------------|
| default     | env.comet.yml                      |
| LOCAL       | env.LOCAL.comet.yml, env.comet.yml |
| BQ          | env.BQ.comet.yml, env.comet.yml    |

Environment is set thanks to `SL_ENV` environment variable. For more details, have a look at the [environment reference](../reference/environment)

In our example, the `root_path` must be defined to the location where the incoming directory containing the datasets to load will be located.
Set it to your project directory as follows in the default environment file:

```
env:
  root_path: ${HOME}/myproject
```

or simply leave it as `{{SL_ROOT}}`.

`SL_ROOT` is a special variable that is set to the directory where the `starlake` command is executed.

## Load files
Loading is a two step process: The optional `import` step and the `load` step.

### Import step
In this first step, Starlake will look at the directory defined in the YAML file and look at the file that matches the expected pattern.
In our example, the directory is `{{root_path}}/incoming` and the expected file pattern has been changed to `customers-.*.psv`
`import` move files from domain directory defined in any domain files such as `metadata/domains/sales.comet.yml` to the `datasets/pending` directory.
Moving files is only done when it matches at least one table pattern otherwise, the file is ignored and moved to the _unresolved_ directory.


<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh import
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd import
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e SL_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake import
```

</TabItem>
</Tabs>

The file has now been moved to the `myproject/datasets/pending/sales` directory.

:::note

This step is optional and does not need to be run if your files directly arrive in the `datasets/pending/sales` folder.
Also note that all these source directories may be [redefined](../reference/configuration).

:::

### Load step

In this second step, each line in the file present in the `datasets/pending` folder is checked against the schema described in the YAML file
and its result is stored in the warehouse.

Before loading data to the warehouse, create a copy of the psv file and name it `unknown-table.psv`. Then run the `load` command to load data as below:

<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh load
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd load
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e SL_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake load
```

</TabItem>
</Tabs>

This will load the dataset and store it as a parquet file into the folders:
- `datasets/accepted` for valid records
- `datasets/rejected` for invalid records
- `datasets/unresolved` for unrecognized files

The latter one is where you should find `unknown-table.psv`.

:::caution

Starlake validate the data against the table's schema of the first pattern that match with the file name.
Hence, you must be careful regarding the pattern you set.
Make sure that there is no overlap.

:::


### Check the result

You can check the result by running the following python script:

```python
import pandas as pd
filepath = 'datasets/accepted/sales/customers/'
df = pd.read_parquet(filepath)
df.head
```

### Loading the data into BigQuery
We just loaded our text file into a parquet file. This is a very common format for data scientists and analysts.
Through minimum extra configuration, we are able to run Starlake on top of any warehouse and have these datasets available as tables.

In our example, we will load the data into a BigQuery table.
define the default env as being BQ in the `metadata/env.comet.yml` file:
```yaml {3,3} title="Define the default environment to activate the env.BQ.comet.yml configuration file"
env:
  root_path: "{{SL_ROOT}}"
  SL_ENV: BQ
  myConnectionRef: "localFilesystem"
  bigQueryMaterializationDataset: "???"
  validator: "???" # native of spark
```

