.. _sink_concept:

Sink
##########################################

Once ingested, files may be sinked to BigQuery, Elasticsearch or any JDBC compliant Database.

.. option:: type: Enum

              - JDBC : dataset will be sinked to a JDBC Database. See JdbcSink below
              - ES : dataset is indexed into Elasticsearch. See EsSink below
              - BQ : Dataset is sinked to BigQuery. See BigQuerySink below
              - None: Don't sink. This is the default.

.. option:: name: String

            This optional name is used when the configuration is specified in the application.conf file instead of inline in the YAML file.
            This is useful when the same sink parameters are used for different datasets.


BigQuerySink
^^^^^^^^^^^^
When the sink type field is set to BQ, the options below shoiuld be provided.
.. option:: location: String
Database location (EU, US, ...)

.. option:: timestamp: String

The timestamp column to use for table partitioning if any. No partitioning by default

.. option:: clustering: List[String]

List of ordered columns to use for table clustering

.. option:: days: Int

Number of days before this table is set as expired and deleted. Never by default.

.. option:: requirePartitionFilter: Boolean

Should be require a partition filter on every request ? No by default.

EsSink
^^^^^^
When the sink *type* field is set to ES, the options below should be provided.
Elasticsearch options are specified in the application.conf file.

.. option:: id: String

Attribute to use as id of the document. Generated by Elasticseach if not specified.

.. option:: timestamp: String

Timestamp field format as expeted by Elasticsearch ("{beginTs|yyyy.MM.dd}" for example).


JdbcSink
^^^^^^^^
When the sink *type* field is set to JDBC, the options below should be provided.

.. option:: connection: String

The JDBC Connection String. Specific to the target JDBC database

.. option:: partitions: Int

Number of Spark partitions

.. option:: batchsize: Int

Batch size of each JDBC bulk insert
