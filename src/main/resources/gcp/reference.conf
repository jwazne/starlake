root = "/tmp"
root = ${?SL_ROOT}

datasets = ${root}"/datasets"
datasets = ${?SL_DATASETS}

metadata = ${root}"/metadata"
metadata = ${?SL_METADATA}

tmpdir = ${root}"/SL_tmp"
tmpdir = ${?SL_TMPDIR}

archive = true
archive = ${?SL_ARCHIVE}

launcher = airflow
launcher = simple
launcher = ${?SL_LAUNCHER}

expectations {
  active = false
  active = ${?SL_EXPECTATIONS_ACTIVE}
  path = ${root}"/expectations/{{domain}}"
  path = ${?SL_EXPECTATIONS_PATH}
  sink {
    type = "DefaultSink" # can be BigQuerySink or JdbcSink or DefaultSink or EsSink
    type = ${?SL_EXPECTATIONS_SINK_TYPE}
    connection-ref = "expectations" // serves as dataset name for BigQuery or Elasticsearch index name

    ## BigQuery options
    # location = "EU"
    # timestamp = "_PARTITIONTIME"
    # clustering = "???"
    # days = 7
    # require-partition-filter = false

    # Jdbc options
    partitions = 1
    batch-size = 1000
  }
}

hive = false
hive = ${?SL_HIVE}

grouped = false
grouped = ${?SL_GROUPED}

analyze = true
analyze = ${?SL_ANALYZE}

# Save Format in CSV with coalesce(1)
csv-output = false
csv-output = ${?SL_CSV_OUTPUT}

privacy-only = false
privacy-only = ${?SL_PRIVACY_ONLY}

default-write-format = parquet
default-write-format = ${?SL_DEFAULT_WRITE_FORMAT}

default-rejected-write-format = parquet
default-rejected-write-format = ${?SL_DEFAULT_REJECTED_WRITE_FORMAT}


merge-force-distinct = false
merge-force-distinct = ${?SL_MERGE_FORCE_DISTINCT}

file-system = "gs://nps-datastore"
#file-system = "hdfs://localhost:9000"
#file-system = "abfs://starlakefs@starlakeacc.dfs.core.windows.net"
file-system = ${?SL_FS}

udfs = ${?SL_UDFS}

metadata-file-system = ${file-system}
metadata-file-system = ${?SL_METADATA_FS}

chewer-prefix = "starlake.chewer"
chewer-prefix = ${?SL_CHEWER_PREFIX}

row-validator-class = "ai.starlake.job.validator.FlatRowValidator"
row-validator-class = ${?SL_ROW_VALIDATOR_CLASS}

lock {
  path = ${root}"/locks"
  path = ${?SL_LOCK_PATH}


  timeout = -1
  timeout = ${?SL_LOCK_TIMEOUT}
}

hadoop {
}

audit {
  path = ${root}"/audit"
  path = ${?SL_AUDIT_PATH}

  audit-timeout = -1
  audit-timeout = ${?SL_LOCK_AUDIT_TIMEOUT}

  max-errors = 100
  sink {
    type = "DefaultSink" # can be BigQuerySink or JdbcSink or DefaultSink or EsSink
    type = ${?SL_AUDIT_SINK_TYPE}
    connection-ref = "audit" // serves as dataset name for BigQuery or Elasticsearch index name
    connection-ref = ${?SL_AUDIT_SINK_NAME}

    ## BigQuery options
    # location = "EU"
    # timestamp = "_PARTITIONTIME"
    # clustering = "???"
    # days = 7
    # require-partition-filter = false


    # Jdbc options
    partitions = 1
    batchSize = 1000
  }

}

metrics {
  active = false
  active = ${?SL_METRICS_ACTIVE}

  #  path = ${root}"/metrics/{domain}/{schema}"
  path = ${root}"/metrics/{{domain}}"
  path = ${?SL_METRICS_PATH}

  discrete-max-cardinality = 10
  discrete-max-cardinality = ${?SL_METRICS_DISCRETE_MAX_CARDINALITY}
}

connections = {
  "audit": {
    "format" = "jdbc"
    #    "mode" = "Append"
    options = {
      "url": "jdbc:postgresql://127.0.0.1:5403/starlake?user=postgres&password=ficpug-Podbid-7fobnu",
      "user": "postgres",
      "password": "ficpug-Podbid-7fobnu",
      "driver": "org.postgresql.Driver"
      # driver = "org.h2.Driver"
    }
  }
}

jdbc-engines {
  h2 {
    tables = {
      "expectations": {
        create-sql = """CREATE TABLE IF NOT EXISTS expectations (
                            name VARCHAR(255) not NULL,
                            params VARCHAR(255) not NULL,
                            sql TEXT,
                            count BIGINT,
                            message TEXT,
                            success BOOLEAN not NULL,
                            jobid VARCHAR(255) not NULL,
                            domain VARCHAR(255) not NULL,
                            schema VARCHAR(255) not NULL,
                            count BIGINT not NULL,
                            cometTime TIMESTAMP not NULL,
                            cometStage VARCHAR(255) not NULL
                             )
    """
      },
      "audit": {
        create-sql = """CREATE TABLE IF NOT EXISTS AUDIT (
                              jobid VARCHAR(255) not NULL,
                              paths VARCHAR(255) not NULL,
                              domain VARCHAR(255) not NULL,
                              schema VARCHAR(255) not NULL,
                              success BOOLEAN not NULL,
                              count BIGINT not NULL,
                              countAccepted BIGINT not NULL,
                              countRejected BIGINT not NULL,
                              timestamp TIMESTAMP not NULL,
                              duration INTEGER not NULL,
                              message VARCHAR(255) not NULL
                             )
    """
      },
      "rejected": {
        create-sql = """CREATE TABLE IF NOT EXISTS REJECTED (
                              jobid VARCHAR(255) not NULL,
                              timestamp TIMESTAMP not NULL,
                              domain VARCHAR(255) not NULL,
                              schema VARCHAR(255) not NULL,
                              error VARCHAR(255) not NULL,
                              path VARCHAR(255) not NULL
                             )
    """
      },
      "frequencies": {
        create-sql = """CREATE TABLE IF NOT EXISTS frequencies (
            attribute VARCHAR(255) not NULL,
            category TEXT NULL,
            count BIGINT not NULL,
            frequency DOUBLE PRECISION NULL,
            jobId VARCHAR(255) not NULL,
            domain VARCHAR(255) not NULL,
            schema VARCHAR(255) not NULL,
            cometTime BIGINT not NULL,
            cometStage VARCHAR(255) not NULL
        )
        """
      },
      "continuous": {
        create-sql = """CREATE TABLE IF NOT EXISTS continuous (
            attribute VARCHAR(255) not NULL,
            min DOUBLE PRECISION NULL,
            max DOUBLE PRECISION NULL,
            mean DOUBLE PRECISION NULL,
            missingValues BIGINT NULL,
            variance DOUBLE PRECISION NULL,
            standardDev DOUBLE PRECISION NULL,
            sum DOUBLE PRECISION NULL,
            skewness DOUBLE PRECISION NULL,
            kurtosis DOUBLE PRECISION NULL,
            percentile25 DOUBLE PRECISION NULL,
            median DOUBLE PRECISION NULL,
            percentile75 DOUBLE PRECISION NULL,
            cometMetric VARCHAR(255) not NULL,
            jobId VARCHAR(255) not NULL,
            domain VARCHAR(255) not NULL,
            schema VARCHAR(255) not NULL,
            count BIGINT not NULL,
            cometTime BIGINT not NULL,
            cometStage VARCHAR(255) not NULL
        )
        """
      },
      "discrete": {
        create-sql = """CREATE TABLE IF NOT EXISTS discrete (
            attribute VARCHAR(255) not NULL,
            countDistinct BIGINT NULL,
            missingValuesDiscrete BIGINT NULL,
            cometMetric VARCHAR(255) not NULL,
            jobId VARCHAR(255) not NULL,
            domain VARCHAR(255) not NULL,
            schema VARCHAR(255) not NULL,
            count BIGINT not NULL,
            cometTime BIGINT not NULL,
            cometStage VARCHAR(255) not NULL
        )
        """
      }
    }
  }

  postgresql {
    tables = {
      "expectations": {
        create-sql = """CREATE TABLE IF NOT EXISTS expectations (
                            name VARCHAR(255) not NULL,
                            params VARCHAR(255) not NULL,
                            sql TEXT,
                            count BIGINT,
                            message TEXT,
                            success BOOLEAN not NULL,
                            jobid VARCHAR(255) not NULL,
                            domain VARCHAR(255) not NULL,
                            schema VARCHAR(255) not NULL,
                            count BIGINT not NULL,
                            cometTime TIMESTAMP not NULL,
                            cometStage VARCHAR(255) not NULL
                             )
    """
      },
      "audit": {
        create-sql = """CREATE TABLE IF NOT EXISTS audit (
                              jobid VARCHAR(255) not NULL,
                              paths VARCHAR(255) not NULL,
                              domain VARCHAR(255) not NULL,
                              schema VARCHAR(255) not NULL,
                              success BOOLEAN not NULL,
                              count BIGINT not NULL,
                              countAccepted BIGINT not NULL,
                              countRejected BIGINT not NULL,
                              timestamp TIMESTAMP not NULL,
                              duration INTEGER not NULL,
                              message VARCHAR(255) not NULL
                             )
    """
      },
      "rejected": {
        create-sql = """CREATE TABLE IF NOT EXISTS rejected (
                              jobid VARCHAR(255) not NULL,
                              timestamp TIMESTAMP not NULL,
                              domain VARCHAR(255) not NULL,
                              schema VARCHAR(255) not NULL,
                              error VARCHAR(255) not NULL,
                              path VARCHAR(255) not NULL
                             )
    """
      },
      "frequencies": {
        create-sql = """CREATE TABLE IF NOT EXISTS frequencies (
            attribute VARCHAR(255) not NULL,
            category TEXT NULL,
            count BIGINT not NULL,
            frequency DOUBLE PRECISION NULL,
            jobId VARCHAR(255) not NULL,
            domain VARCHAR(255) not NULL,
            schema VARCHAR(255) not NULL,
            cometTime BIGINT not NULL,
            cometStage VARCHAR(255) not NULL
        )
        """
      },
      "continuous": {
        create-sql = """CREATE TABLE IF NOT EXISTS continuous (
            attribute VARCHAR(255) not NULL,
            min DOUBLE PRECISION NULL,
            max DOUBLE PRECISION NULL,
            mean DOUBLE PRECISION NULL,
            missingValues BIGINT NULL,
            variance DOUBLE PRECISION NULL,
            standardDev DOUBLE PRECISION NULL,
            sum DOUBLE PRECISION NULL,
            skewness DOUBLE PRECISION NULL,
            kurtosis DOUBLE PRECISION NULL,
            percentile25 DOUBLE PRECISION NULL,
            median DOUBLE PRECISION NULL,
            percentile75 DOUBLE PRECISION NULL,
            cometMetric VARCHAR(255) not NULL,
            jobId VARCHAR(255) not NULL,
            domain VARCHAR(255) not NULL,
            schema VARCHAR(255) not NULL,
            count BIGINT not NULL,
            cometTime BIGINT not NULL,
            cometStage VARCHAR(255) not NULL
        )
        """
      },
      "discrete": {
        name = "discrete"
        create-sql = """CREATE TABLE IF NOT EXISTS discrete (
            attribute VARCHAR(255) not NULL,
            countDistinct BIGINT NULL,
            missingValuesDiscrete BIGINT NULL,
            cometMetric VARCHAR(255) not NULL,
            jobId VARCHAR(255) not NULL,
            domain VARCHAR(255) not NULL,
            schema VARCHAR(255) not NULL,
            count BIGINT not NULL,
            cometTime BIGINT not NULL,
            cometStage VARCHAR(255) not NULL
        )
        """
      }
    }
  }
}

area {
  pending = "pending"
  pending = ${?SL_AREA_PENDING}
  unresolved = "unresolved"
  unresolved = ${?SL_AREA_UNRESOLVED}
  archive = "archive"
  archive = ${?SL_AREA_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?SL_AREA_INGESTING}
  accepted = "accepted"
  accepted = ${?SL_AREA_ACCEPTED}
  rejected = "rejected"
  rejected = ${?SL_AREA_REJECTED}
  business = "business"
  business = ${?SL_AREA_BUSINESS}
}

privacy {
  options = {
    "none": "ai.starlake.privacy.No",
    "hide": "ai.starlake.privacy.Hide",
    "hide10X": "ai.starlake.privacy.Hide(\"X\",10)",
    "approxLong20": "ai.starlake.privacy.ApproxLong(20)",
    "md5": "ai.starlake.privacy.Md5",
    "sha1": "ai.starlake.privacy.Sha1",
    "sha256": "ai.starlake.privacy.Sha256",
    "sha512": "ai.starlake.privacy.Sha512",
    "initials": "ai.starlake.privacy.Initials"
  }
}

spark {
  #  sql.hive.convertMetastoreParquet = false
  #  yarn.principal = "invalid"
  #  yarn.keytab = "invalid"
  #  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
  #  yarn.keytab = ${?SPARK_YARN_KEYTAB}
  debug.maxToStringFields = 100
  #master = "local[*]"
  #  sql.catalogImplementation="hive"
  #  sql.catalogImplementation="in-memory"
  sql.legacy.parquet.datetimeRebaseModeInWrite = "CORRECTED"
  viewsEnabled = "true"
}


# curl -v -H 'Cache-Control: no-cache'  -H 'Content-Type: application/json'  -XPOST localhost:8080/api/experimental/dags/SL_validator/dag_runs -d '{"conf":"{\"key\":\"value\"}"}'

airflow {
  ingest = "SL_ingest"
  ingest = ${?AIRFLOW_INGEST}
  endpoint = "http://127.0.0.1:8080/api/experimental"
  endpoint = ${?AIRFLOW_ENDPOINT}
}


internal {
  # See https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html
  cache-storage-level = "MEMORY_AND_DISK"
  cache-storage-level = ${?SL_INTERNAL_CACHE_STORAGE_LEVEL}
}

}
