internal {
  # See https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html
  cacheStorageLevel = "MEMORY_AND_DISK"
  cacheStorageLevel = ${?SL_INTERNAL_CACHE_STORAGE_LEVEL}
  # Parquet is the default format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet
  # other values include orc & avro. When using avro, spark-avro dependency should be included at runtime.
  # If you want to use orc or avro format, you should be aware of this
  # For orc format (All fields in the detected schema are NULLABLE) : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc
  # For avro format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro
  intermediateBigqueryFormat = "parquet"
  intermediateBigqueryFormat = ${?SL_INTERMEDIATE_BQ_FORMAT}
  substituteVars = true
  substituteVars = ${?SL_INTERNAL_SUBSTITUTE_VARS}
  temporaryGcsBucket = ${?TEMPORARY_GCS_BUCKET}
  temporaryGcsBucket = ${?SL_TEMPORARY_GCS_BUCKET}

  bqAuditSaveInBatchMode = true
  bqAuditSaveInBatchMode = ${?SL_BQ_AUDIT_SAVE_IN_BATCH_MODE}
}
