# file-system / SL_FS: Filesystem to use for all relative paths
file-system = "file://"
file-system = ${?COMET_FS}
file-system = ${?SL_FS}

# validate-on-load / SL_VALIDATE_ON_LOAD: Validate all comet.yml files on load and stop at least one is invalid
validate-on-load = false
validate-on-load = ${?COMET_VALIDATE_ON_LOAD}
validate-on-load = ${?SL_VALIDATE_ON_LOAD}

tenant="" # tenant name (optional)
tenant=${?SL_TENANT}

project="" # project name (optional)
project=${?SL_PROJECT}

# root / SL_ROOT: Root path for all starlake relatibve paths
root = "/tmp"
root = ${?COMET_ROOT}
root = ${?SL_ROOT}


root-serve = ${?COMET_ROOT_SERVE}
root-serve = ${?SL_ROOT_SERVE}

# session-duration-serve / SL_SESSION_DURATION_SERVE: Session duration in minutes for starlae server
session-duration-serve = 10 # in minutes
session-duration-serve = ${?COMET_SESSION_DURATION_SERVE}
session-duration-serve = ${?SL_SESSION_DURATION_SERVE}

# datasets / SL_DATASETS: Datasets path, may be relative or absolute
datasets = ${root}"/datasets"
datasets = ${?COMET_DATASETS}
datasets = ${?SL_DATASETS}

# metadata / SL_METADATA: Metadata path, may be relative or absolute
metadata = ${root}"/metadata"
metadata = ${?COMET_METADATA}
metadata = ${?SL_METADATA}

# dags / SL_DAGS: Dags path, may be relative or absolute
dags = ${root}"/dags"
dags = ${?COMET_DAGS}
dags = ${?SL_DAGS}

# use-local-file-system / SL_USE_LOCAL_FILE_SYSTEM: Do not use Hadoop HDFS path abstraction, use java file API  instead
use-local-file-system = false
use-local-file-system = ${?COMET_USE_LOCAL_FILE_SYSTEM}
use-local-file-system = ${?SL_USE_LOCAL_FILE_SYSTEM}


dsv-options {
  # any option listed here https://spark.apache.org/docs/latest/sql-data-sources-csv.html
}



area {
  pending = "pending"
  pending = ${?COMET_AREA_PENDING}
  pending = ${?SL_AREA_PENDING}
  unresolved = "unresolved"
  unresolved = ${?COMET_AREA_UNRESOLVED}
  unresolved = ${?SL_AREA_UNRESOLVED}
  archive = "archive"
  archive = ${?COMET_AREA_ARCHIVE}
  archive = ${?SL_AREA_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?COMET_AREA_INGESTING}
  ingesting = ${?SL_AREA_INGESTING}
  accepted = "accepted"
  accepted = ${?COMET_AREA_ACCEPTED}
  accepted = ${?SL_AREA_ACCEPTED}
  rejected = "rejected"
  rejected = ${?COMET_AREA_REJECTED}
  rejected = ${?SL_AREA_REJECTED}
  business = "business"
  business = ${?COMET_AREA_BUSINESS}
  business = ${?SL_AREA_BUSINESS}
  replay = "replay"
  replay = ${?COMET_AREA_REPLAY}
  replay = ${?SL_AREA_REPLAY}
  hive-database = "${domain}_${area}"
  hive-database = ${?COMET_AREA_HIVE_DATABASE}
  hive-database = ${?SL_AREA_HIVE_DATABASE}
}

tmpdir = ${root}"/comet_tmp"
tmpdir = ${?COMET_TMPDIR}
tmpdir = ${?SL_TMPDIR}

archive = true
archive = ${?COMET_ARCHIVE}
archive = ${?SL_ARCHIVE}

default-file-extensions = "json,csv,dsv,psv,dat,txt,xml"
default-file-extensions = ${?COMET_DEFAULT_FILE_EXTENSIONS}
default-file-extensions = ${?SL_DEFAULT_FILE_EXTENSIONS}

force-file-extensions =  "json,csv,dsv,psv,dat,txt,xml,plt"
force-file-extensions = ${?COMET_FORCE_FILE_EXTENSIONS}
force-file-extensions = ${?SL_FORCE_FILE_EXTENSIONS}

default-format = parquet
default-format = ${?COMET_DEFAULT_WRITE_FORMAT}
default-format = ${?SL_DEFAULT_WRITE_FORMAT}
default-format = ${?COMET_DEFAULT_FORMAT}
default-format = ${?SL_DEFAULT_FORMAT}

default-rejected-write-format = parquet
default-rejected-write-format = ${?COMET_DEFAULT_REJECTED_WRITE_FORMAT}
default-rejected-write-format = ${?SL_DEFAULT_REJECTED_WRITE_FORMAT}

default-audit-write-format = parquet
default-audit-write-format = ${?COMET_DEFAULT_AUDIT_WRITE_FORMAT}
default-audit-write-format = ${?SL_DEFAULT_AUDIT_WRITE_FORMAT}

reject-all-on-error = false
reject-all-on-error = ${?COMET_REJECT_ALL_ON_ERROR}
reject-all-on-error = ${?SL_REJECT_ALL_ON_ERROR}

reject-max-records = 1000000

hive = false
hive = ${?COMET_HIVE}
hive = ${?SL_HIVE}


analyze = true
analyze = ${?COMET_ANALYZE}
analyze = ${?SL_ANALYZE}

launcher = airflow
launcher = simple
launcher = ${?COMET_LAUNCHER}
launcher = ${?SL_LAUNCHER}

lock {
  path = ${root}"/locks"
  path = ${?COMET_LOCK_PATH}
  path = ${?SL_LOCK_PATH}

  timeout = -1
  time-out = ${?COMET_LOCK_TIMEOUT}
  time-out = ${?SL_LOCK_TIMEOUT}
}

grouped = false
grouped = ${?COMET_GROUPED}
grouped = ${?SL_GROUPED}

grouped-max = 1000000
grouped-max = ${?COMET_GROUPED_MAX}
grouped-max = ${?SL_GROUPED_MAX}

load-strategy-class = "ai.starlake.job.load.IngestionTimeStrategy"
load-strategy-class = ${?COMET_LOAD_STRATEGY}
load-strategy-class = ${?SL_LOAD_STRATEGY}

sink-to-file = false
sink-to-file = ${?COMET_SINK_TO_FILE}
sink-to-file = ${?SL_SINK_TO_FILE}

sink-replay-to-file = false
sink-replay-to-file = ${?COMET_SINK_REPLAY_TO_FILE}
sink-replay-to-file = ${?SL_SINK_REPLAY_TO_FILE}

# Save Format in CSV with coalesce(1)
csv-output = false
csv-output = ${?COMET_CSV_OUTPUT}
csv-output = ${?SL_CSV_OUTPUT}

csv-output-ext = ""
csv-output-ext = ${?COMET_CSV_OUTPUT_EXT}
csv-output-ext = ${?SL_CSV_OUTPUT_EXT}

max-par-copy = 1

scheduling {
  max-jobs = 1
  mode = "FIFO"
  file = ""
  pool-name = "default"
}

privacy {
  options {
    "none": "ai.starlake.privacy.No",
    "hide": "ai.starlake.privacy.Hide",
    "hide10X": "ai.starlake.privacy.Hide(\"X\",10)",
    "approxLong20": "ai.starlake.privacy.ApproxLong(20)",
    "md5": "ai.starlake.privacy.Md5",
    "sha1": "ai.starlake.privacy.Sha1",
    "sha256": "ai.starlake.privacy.Sha256",
    "sha512": "ai.starlake.privacy.Sha512",
    "initials": "ai.starlake.privacy.Initials"
  }
}

privacy-only = false
privacy-only = ${?COMET_PRIVACY_ONLY}
privacy-only = ${?SL_PRIVACY_ONLY}

merge-force-distinct = false
merge-force-distinct = ${?COMET_MERGE_FORCE_DISTINCT}
merge-force-distinct = ${?SL_MERGE_FORCE_DISTINCT}

merge-optimize-partition-write = false
merge-optimize-partition-write = ${?COMET_MERGE_OPTIMIZE_PARTITION_WRITE}
merge-optimize-partition-write = ${?SL_MERGE_OPTIMIZE_PARTITION_WRITE}

udfs = ${?COMET_UDFS}
udfs = ${?SL_UDFS}

chewer-prefix = "comet.chewer"
chewer-prefix = ${?COMET_CHEWER_PREFIX}
chewer-prefix = ${?SL_CHEWER_PREFIX}

empty-is-null = true
empty-is-null = ${?COMET_EMPTY_IS_NULL}
empty-is-null = ${?SL_EMPTY_IS_NULL}

row-validator-class = "ai.starlake.job.validator.FlatRowValidator"
row-validator-class = ${?COMET_ROW_VALIDATOR_CLASS}
row-validator-class = ${?SL_ROW_VALIDATOR_CLASS}

tree-validator-class = "ai.starlake.job.validator.TreeRowValidator"
tree-validator-class = ${?COMET_TREE_VALIDATOR_CLASS}
tree-validator-class = ${?SL_TREE_VALIDATOR_CLASS}

env = prod
env = ${?COMET_ENV}
env = ${?SL_ENV}
database = ""


sql-parameter-pattern = "\\$\\{\\s*%s\\s*\\}"

hadoop {
}

access-policies {
  apply = true
  apply = ${?COMET_ACCESS_POLICIES_APPLY}
  apply = ${?SL_ACCESS_POLICIES_APPLY}
  location = "invalid_location" // eu or us or ...
  location = ${?COMET_ACCESS_POLICIES_LOCATION}
  location = ${?SL_ACCESS_POLICIES_LOCATION}
  project-id = "invalid_project"
  project-id = ${?GCP_PROJECT}
  project-id = ${?GCLOUD_PROJECT}
  project-id = ${?COMET_ACCESS_POLICIES_PROJECT_ID}
  project-id = ${?SL_ACCESS_POLICIES_PROJECT_ID}

  taxonomy = "invalid_taxonomy"
  taxonomy = ${?COMET_ACCESS_POLICIES_TAXONOMY}
  taxonomy = ${?SL_ACCESS_POLICIES_TAXONOMY}
}


force-view-pattern = "[a-zA-Z][a-zA-Z0-9_]{1,256}"
force-view-pattern = ${?COMET_FORCE_VIEW_PATTERN}
force-view-pattern = ${?SL_FORCE_VIEW_PATTERN}
force-domain-pattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
force-domain-pattern = ${?COMET_FORCE_DOMAIN_PATTERN}
force-domain-pattern = ${?SL_FORCE_DOMAIN_PATTERN}
force-table-pattern = "[a-zA-Z][a-zA-Z0-9_]{1,256}"
force-table-pattern = ${?COMET_FORCE_TABLE_PATTERN}
force-table-pattern = ${?SL_FORCE_TABLE_PATTERN}
force-job-pattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
force-job-pattern = ${?COMET_FORCE_JOB_PATTERN}
force-job-pattern = ${?SL_FORCE_JOB_PATTERN}
force-task-pattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
force-task-pattern = ${?COMET_FORCE_TASK_PATTERN}
force-task-pattern = ${?SL_FORCE_TASK_PATTERN}

include "reference-audit"
include "reference-metrics"
include "reference-assertions"
include "reference-elasticsearch"
include "reference-atlas"
include "reference-spark"
include "reference-airflow"
include "reference-kafka"
include "reference-connections"
include "reference-service"


internal {
  # See https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html
  cache-storage-level = "MEMORY_AND_DISK"
  cache-storage-level = ${?COMET_INTERNAL_CACHE_STORAGE_LEVEL}
  cache-storage-level = ${?SL_INTERNAL_CACHE_STORAGE_LEVEL}
  # Parquet is the default format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet
  # other values include orc & avro. When using avro, spark-avro dependency should be included at runtime.
  # If you want to use orc or avro format, you should be aware of this
  # For orc format (All fields in the detected schema are NULLABLE) : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc
  # For avro format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro
  intermediate-bigquery-format = "parquet"
  intermediate-bigquery-format = ${?COMET_INTERMEDIATE_BQ_FORMAT}
  intermediate-bigquery-format = ${?SL_INTERMEDIATE_BQ_FORMAT}
  substitute-vars = true
  substitute-vars = ${?COMET_INTERNAL_SUBSTITUTE_VARS}
  substitute-vars = ${?SL_INTERNAL_SUBSTITUTE_VARS}
  temporary-gcs-bucket = ${?TEMPORARY_GCS_BUCKET}
  temporary-gcs-bucket = ${?COMET_TEMPORARY_GCS_BUCKET}
  temporary-gcs-bucket = ${?SL_TEMPORARY_GCS_BUCKET}
}

extra {

}

