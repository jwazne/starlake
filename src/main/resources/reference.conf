root = "/tmp"
root = ${?COMET_ROOT}

datasets = ${root}"/datasets"
datasets = ${?COMET_DATASETS}

metadata = ${root}"/metadata"
metadata = ${?COMET_METADATA}

tmpdir = ${root}"/comet_tmp"
tmpdir = ${?COMET_TMPDIR}

archive = true
archive = ${?COMET_ARCHIVE}

launcher = airflow
launcher = simple
launcher = ${?COMET_LAUNCHER}

env = prod
env = ${?COMET_ENV}

sink-to-file = true
sink-to-file = ${?COMET_SINK_TO_FILE}

assertions {
  active = false
  active = ${?COMET_ASSERTIONS_ACTIVE}
  path = ${root}"/assertions/{domain}"
  path = ${?COMET_ASSERTIONS_PATH}
  sink {
    type = "NoneSink" # can be BigQuerySink or JdbcSink or NoneSink or EsSink
    type = ${?COMET_ASSERTIONS_SINK_TYPE}
    name = "assertions" // serves as dataset name for BigQuery or Elasticsearch index name
    ## BigQuery options
    # location = "EU"
    # timestamp = "_PARTITIONTIME"
    # clustering = "???"
    # days = 7
    # require-partition-filter = false
    options {
      allowFieldAddition: "true"
      allowFieldRelaxation: "true"
    }

    ## Jdbc options
    #connection = "assertions"
    #partitions = 1
    #batch-size = 1000
  }
}

hive = false
hive = ${?COMET_HIVE}

grouped = false
grouped = ${?COMET_GROUPED}

analyze = true
analyze = ${?COMET_ANALYZE}

# Save Format in CSV with coalesce(1)
csv-output = false
csv-output = ${?COMET_CSV_OUTPUT}

privacy-only = false
privacy-only = ${?COMET_PRIVACY_ONLY}

default-write-format = parquet
default-write-format = ${?COMET_DEFAULT_WRITE_FORMAT}

merge-force-distinct = false
merge-force-distinct = ${?COMET_MERGE_FORCE_DISTINCT}

file-system = "file://"
file-system = ${?COMET_FS}

udfs = ${?COMET_UDFS}

metadata-file-system = ${file-system}
metadata-file-system = ${?COMET_METADATA_FS}

chewer-prefix = "comet.chewer"
chewer-prefix = ${?COMET_CHEWER_PREFIX}

row-validator-class = "com.ebiznext.comet.job.ingest.DsvIngestionUtil"
row-validator-class = ${?COMET_ROW_VALIDATOR_CLASS}

lock {
  path = ${root}"/locks"
  path = ${?COMET_LOCK_PATH}

  ingestion-timeout = -1
  ingestion-timeout = ${?COMET_LOCK_INGESTION_TIMEOUT}

  metrics-timeout = -1
  metrics-timeout = ${?COMET_LOCK_METRICS_TIMEOUT}
}

hadoop {
}

audit {
  path = ${root}"/audit"
  path = ${?COMET_AUDIT_PATH}

  audit-timeout = -1
  audit-timeout = ${?COMET_LOCK_AUDIT_TIMEOUT}

  max-errors = 100
  sink {
    type = "NoneSink" # can be BigQuerySink or JdbcSink or NoneSink or EsSink
    type = ${?COMET_AUDIT_SINK_TYPE}
    name = "audit" // serves as dataset name for BigQuery or Elasticsearch index name
    ## BigQuery options
    # location = "EU"
    # timestamp = "_PARTITIONTIME"
    # clustering = "???"
    # days = 7
    # require-partition-filter = false
    options {
      allowFieldAddition: "true"
      allowFieldRelaxation: "true"
    }



    # Jdbc options
    connection = "audit"
    partitions = 1
    batchSize = 1000
  }

}

metrics {
  active = false
  active = ${?COMET_METRICS_ACTIVE}

  #  path = ${root}"/metrics/{domain}/{schema}"
  path = ${root}"/metrics/{domain}"
  path = ${?COMET_METRICS_PATH}

  discrete-max-cardinality = 10
  discrete-max-cardinality = ${?COMET_METRICS_DISCRETE_MAX_CARDINALITY}

  sink {
    type = "NoneSink" # can be BigQuerySink or JdbcSink or NoneSink or EsSink
    type = ${?COMET_METRICS_SINK_TYPE}
    name = "metrics" # serves as dataset name for BigQuery or Elasticsearch index name

    ## BigQuery options
    # location = "EU"
    # timestamp = "_PARTITIONTIME"
    # clustering = "???"
    # days = 7
    # require-partition-filter = false
    options {
      allowFieldAddition: "true"
      allowFieldRelaxation: "true"
    }

    # Jdbc options
    connection = "metrics"
    partitions = 1
    batch-size = 1000
  }
}

area {
  pending = "pending"
  pending = ${?COMET_AREA_PENDING}
  unresolved = "unresolved"
  unresolved = ${?COMET_AREA_UNRESOLVED}
  archive = "archive"
  archive = ${?COMET_AREA_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?COMET_AREA_INGESTING}
  accepted = "accepted"
  accepted = ${?COMET_AREA_ACCEPTED}
  rejected = "rejected"
  rejected = ${?COMET_AREA_REJECTED}
  business = "business"
  business = ${?COMET_AREA_BUSINESS}
}

privacy {
  options {
    "none": "com.ebiznext.comet.privacy.No",
    "hide": "com.ebiznext.comet.privacy.Hide",
    "hide10X": "com.ebiznext.comet.privacy.Hide(\"X\",10)",
    "approxLong20": "com.ebiznext.comet.privacy.ApproxLong(20)",
    "md5": "com.ebiznext.comet.privacy.Md5",
    "sha1": "com.ebiznext.comet.privacy.Sha1",
    "sha256": "com.ebiznext.comet.privacy.Sha256",
    "sha512": "com.ebiznext.comet.privacy.Sha512",
    "initials": "com.ebiznext.comet.privacy.Initials"
  }
}



include "reference-elasticsearch"
include "reference-atlas"
include "reference-spark"
include "reference-airflow"
include "reference-kafka"
include "reference-jdbc"


internal {
  # See https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html
  cache-storage-level = "MEMORY_AND_DISK"
  cache-storage-level = ${?COMET_INTERNAL_CACHE_STORAGE_LEVEL}
  # Parquet is the default format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet
  # other values include orc & avro. When using avro, spark-avro dependency should be included at runtime.
  # If you want to use orc or avro format, you should be aware of this
  # For orc format (All fields in the detected schema are NULLABLE) : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc
  # For avro format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro
  intermediate-bigquery-format = "parquet"
  intermediate-bigquery-format = ${?COMET_INTERMEDIATE_BQ_FORMAT}
}

