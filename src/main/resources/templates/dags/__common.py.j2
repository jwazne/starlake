import re
import os
import json
from datetime import timedelta, datetime

from airflow.models import Variable

from airflow.operators.dummy import DummyOperator

from airflow.utils.trigger_rule import TriggerRule

def keep_ascii_only(text):
    return re.sub(r'[^\x00-\x7F]+', '_', text)

def sanitize_id(id: str):
    return keep_ascii_only(re.sub("[^a-zA-Z0-9\-_]", "_", id.replace("$", "S")))

class MissingEnvironmentVariable(Exception):
    pass

def get_context_var(var_name: str, default_value: any=None, options: dict = None, **kwargs):
    if options.get(var_name):
        return options.get(var_name)
    elif default_value is not None:
        return default_value
    elif Variable.get(var_name, default_var=None, **kwargs) is not None:
        return Variable.get(var_name)
    elif os.getenv(var_name) is not None:
        return os.getenv(var_name)
    else:
        raise MissingEnvironmentVariable(f"{var_name} does not exist")

description="""{{ context.config.comment }}"""

template="{{ context.config.template }}"

options={
    {% for option in context.config.options %}'{{ option.name }}':'{{ option.value }}'{% if not loop.last  %}, {% endif %}
    {% endfor %}
}

# SL_ENV_VARS is a map of env vars to add to the cloud run job ot the dataproc cluster properties
def get_sl_env_vars() -> dict:
    try:
        return json.loads(get_context_var(var_name="sl_env_var", options=options))
    except MissingEnvironmentVariable:
        return {}

SL_ENV_VARS = get_sl_env_vars()

SL_ROOT = get_context_var(
    var_name='SL_ROOT', 
    default_value='file://tmp', 
    options=SL_ENV_VARS
)

SL_DATASETS = get_context_var(
    var_name='SL_DATASETS', 
    default_value=SL_ROOT.join('/datasets/pending'), 
    options=SL_ENV_VARS
)

DEFAULT_POOL="default_pool"

DEFAULT_DAG_ARGS = {
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1, 
    'retry_delay': timedelta(minutes=5),
    'project_id': "${project_id}"
}

TODAY = datetime.today().strftime('%Y-%m-%d')

def dummy_op(task_id, trigger_rule=TriggerRule.ALL_SUCCESS, pool=DEFAULT_POOL):
    return DummyOperator(task_id=task_id, trigger_rule=trigger_rule, pool=pool)

DEFAULT_DAG_NAME=os.path.basename(__file__).replace(".py", "").replace(".pyc", "").lower()
