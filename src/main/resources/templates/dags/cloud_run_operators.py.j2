{% include 'dags/templates/common.py.j2' %}

from airflow.operators.bash import BashOperator
from airflow.sensors.bash import BashSensor
from airflow.datasets import Dataset
from airflow.utils.task_group import TaskGroup

project_id = get_context_var(var_name='cloud_run_project_id', options=options)
cloud_run_job_name = get_context_var(var_name='cloud_run_job_name', options=options)
cloud_run_job_region = get_context_var(var_name='cloud_run_job_region', options=options)

class StarlakeCloudRunJobAsyncOperator(BashOperator):
    '''
    This operator executes a cloud run job and does not wait for its completion.
    '''
    def __init__(self, *, project_id: str, cloud_run_job_name: str, cloud_run_job_region: str, sl_cmd: str, sl_env_vars: dict = {}, **kwargs) -> None:
        update_env_vars = ",".join([("--update-env-vars " if i == 0 else "") + f"{key}='{value}'" for i, (key, value) in enumerate(sl_env_vars.items())])
        super().__init__(
            bash_command=(
                f"gcloud beta run jobs execute {cloud_run_job_name} "
                f"--args '{sl_cmd}' "
                f"{update_env_vars} "
                f"--region {cloud_run_job_region} --project {project_id} --task-timeout 300 --format='get(metadata.name)'"
            ),
            do_xcom_push=True,
            **kwargs
        )

def importJob(task_id: str, project_id: str, cloud_run_job_name: str, cloud_run_job_region: str, domain: str, sl_env_vars: dict = {}, pool: str=DEFAULT_POOL, **kwargs):
    return StarlakeCloudRunJobAsyncOperator(
        task_id=task_id, 
        project_id=project_id, 
        cloud_run_job_name=cloud_run_job_name, 
        cloud_run_job_region=cloud_run_job_region, 
        sl_cmd=f"import --include {domain}",
        sl_env_vars=sl_env_vars,
        pool=pool,
        **kwargs
    )

def loadJob(task_id: str, project_id: str, cloud_run_job_name: str, cloud_run_job_region: str, domain: str, table: str, sl_env_vars: dict = {}, pool: str=DEFAULT_POOL, **kwargs):
    return StarlakeCloudRunJobAsyncOperator(
        task_id=task_id, 
        project_id=project_id, 
        cloud_run_job_name=cloud_run_job_name, 
        cloud_run_job_region=cloud_run_job_region, 
        sl_cmd=f"load --include {domain} --schemas {table}",
        sl_env_vars=sl_env_vars,
        pool=pool,
        **kwargs
    )

def transformJob(task_id: str, project_id: str, cloud_run_job_name: str, cloud_run_job_region: str, task_name: str, sl_env_vars: dict = {}, pool: str=DEFAULT_POOL, **kwargs):
    return StarlakeCloudRunJobAsyncOperator(
        task_id=task_id, 
        project_id=project_id, 
        cloud_run_job_name=cloud_run_job_name, 
        cloud_run_job_region=cloud_run_job_region, 
        sl_cmd=f"transform --name {task_name}",
        sl_env_vars=sl_env_vars,
        pool=pool,
        **kwargs
    )

class CloudRunJobCompletionSensor(BashSensor):
    '''
    This sensor checks the completion of a cloud run job.
    '''
    def __init__(self, *, project_id: str, cloud_run_job_region: str, source_task_id: str, retry_exit_code: int=None, outlets: str, **kwargs) -> None:
        if(retry_exit_code):
            super().__init__(
                bash_command=(f"value=`gcloud beta run jobs executions describe {{"{{{{ task_instance.xcom_pull(task_ids='{source_task_id}') }}}}"}}  --region {cloud_run_job_region} --project {project_id} --format='value(status.failedCount, status.cancelledCounts)' | sed 's/[[:blank:]]//g'`; test -z \"$value\""),
                mode="reschedule",
                retry_exit_code=retry_exit_code, #available in 2.6. Implies to combine this sensor and the bottom operator
                outlets=[Dataset(keep_ascii_only(outlets))],
                **kwargs
            )
        else:
            super().__init__(
                bash_command=(f"value=`gcloud beta run jobs executions describe {{"{{{{ task_instance.xcom_pull(task_ids='{source_task_id}') }}}}"}}  --region {cloud_run_job_region} --project {project_id} --format='value(status.completionTime, status.cancelledCounts)' | sed 's/[[:blank:]]//g'`; test -n \"$value\""),
                mode="reschedule",
                #retry_exit_code=1, #available in 2.6. Implies to combine this sensor and the bottom operator
                **kwargs
            )

class CloudRunJobCheckStatusOperator(BashOperator):
    '''
    This operator checks the status of a cloud run job and fails if it is not successful.
    '''
    def __init__(self, *, project_id: str, cloud_run_job_region: str, source_task_id: str, outlets: str, **kwargs) -> None:
        super().__init__(
            bash_command=(f"value=`gcloud beta run jobs executions describe {{"{{{{ task_instance.xcom_pull(task_ids='{source_task_id}') }}}}"}} --region {cloud_run_job_region} --project {project_id} --format='value(status.failedCount, status.cancelledCounts)' | sed 's/[[:blank:]]//g'`; test -z \"$value\""),
            outlets=[Dataset(keep_ascii_only(outlets))],
            **kwargs
        )

def jobCompletionSensors(source_task_id: str, project_id: str, cloud_run_job_region: str, outlets: str, retry_exit_code: int = None, pool: str=DEFAULT_POOL, **kwargs):
    check_completion_id = source_task_id + '_check_completion'
    completion_sensor = CloudRunJobCompletionSensor(
        task_id=check_completion_id, 
        project_id=project_id, 
        cloud_run_job_region=cloud_run_job_region, 
        source_task_id=source_task_id,
        retry_exit_code=retry_exit_code,
        outlets=outlets,
        pool=pool,
        **kwargs
    )
    if(retry_exit_code):
        return completion_sensor
    else:
        with TaskGroup(group_id=f'{source_task_id}_wait') as task_completion_sensors:
            get_completion_status_id = source_task_id + '_get_completion_status'
            job_status = CloudRunJobCheckStatusOperator(
                task_id=get_completion_status_id, 
                project_id=project_id, 
                cloud_run_job_region=cloud_run_job_region, 
                source_task_id=source_task_id, 
                outlets=outlets, 
                pool=pool,
                **kwargs
            )
            completion_sensor >> job_status
        return task_completion_sensors
