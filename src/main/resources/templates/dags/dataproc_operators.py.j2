import uuid
import os
import ast
from datetime import datetime

from airflow.models import Variable

from airflow.providers.google.cloud.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocSubmitJobOperator,
    DataprocDeleteClusterOperator,
)

from airflow.utils.trigger_rule import TriggerRule

{% include 'dags/templates/common.py.j2' %}

# Spark Job arguments
MAIN_JAR = "gs://${artefacts_bucket}/${main_jar}"
SPARK_BQ_JAR = "gs://${artefacts_bucket}/com.google.cloud.spark/spark-bigquery-with-dependencies_2.12/${spark_bq_version}/spark-bigquery-with-dependencies_2.12-${spark_bq_version}.jar"
JARS_LIST = [MAIN_JAR, SPARK_BQ_JAR]
MAIN_CLASS = "ai.starlake.job.Main"
OUTGOING_MAIN = "com.natixis.gi6.utils.CopyFromDatastoreToOutgoingZipped"
EXPORT_MAIN = "com.natixis.gi6.bigquery.DataExporter"
MAIN_MOVE_FILES = "com.natixis.gi6.utils.HandleFiles"

# GCP parameters
PROJECT_ID = "${project_id}"
ZONE = "${zone}"
REGION = "${region}"
SUBNET = "${subnet}"

CLUSTER_SIZE = "${dataproc_cluster_size}"

TODAY = datetime.today().strftime('%Y-%m-%d')

# Dataproc parameters
DATAPROC_SERVICE_ACCOUNT = "${dataproc_service_account}"
IMAGE_VERSION = "${dataproc_image_version}"
MASTER_MACHINE_TYPE = "${dataproc_master_machine_type}"
WORKER_MACHINE_TYPE = "${dataproc_worker_machine_type}"

DEFAULT_MASTER_CONF = {
    "num_instances": 1,
    "machine_type_uri": MASTER_MACHINE_TYPE,
    "disk_config": {"boot_disk_type": "pd-standard", "boot_disk_size_gb": 1024},
}

DEFAULT_WORKER_CONF = {
    "num_instances": int(CLUSTER_SIZE),
    "machine_type_uri": WORKER_MACHINE_TYPE,
    "disk_config": {"boot_disk_type": "pd-standard", "boot_disk_size_gb": 1024},
}

DEFAULT_SPARK_CONF = {
    "memAlloc":"22g",
    "numVcpu":8,
    "sparkExecutorInstances":3
}

DEFAULT_SPARK_PROPERTIES = {
    "spark.hadoop.fs.defaultFS": "gs://${datastore_bucket}",
    "spark.eventLog.enabled": "true",
    "spark.executor.memory": DEFAULT_SPARK_CONF.get("memAlloc"),
    "spark.executor.cores": DEFAULT_SPARK_CONF.get("numVcpu"),
    "spark.executor.instances": str(DEFAULT_SPARK_CONF.get("sparkExecutorInstances")),
    "spark.sql.sources.partitionOverwriteMode": "DYNAMIC",
    "spark.sql.legacy.parquet.int96RebaseModeInWrite": "CORRECTED",
    "spark.sql.catalogImplementation": "in-memory",
    "spark.datasource.bigquery.temporaryGcsBucket": "${datastore_bucket}",
    "spark.datasource.bigquery.allowFieldAddition": "true",
    "spark.datasource.bigquery.allowFieldRelaxation": "true",
    "spark.dynamicAllocation.enabled": "false",
    "spark.shuffle.service.enabled": "false"
}

SPARK_EVENTS_BUCKET = f'dataproc-{PROJECT_ID}'

COMET_GROUPED = Variable.get("COMET_GROUPED", "true")
#COMET_SINK_TO_FILE = Variable.get("COMET_SINK_TO_FILE", "false")
#COMET_SINK_AUDIT_TO_FILE = Variable.get("COMET_SINK_AUDIT_TO_FILE", "false")

DEFAULT_CLUSTER_PROPERTIES = {
    "dataproc:dataproc.logging.stackdriver.job.driver.enable" : "true",
    "dataproc:dataproc.logging.stackdriver.job.yarn.container.enable": "true",
    "dataproc:dataproc.logging.stackdriver.enable": "true",
    "dataproc:jobs.file-backed-output.enable": "true",
    "dataproc:job.history.to-gcs.enabled": "true",
    "spark:spark.history.fs.logDirectory": f"gs://{SPARK_EVENTS_BUCKET}/tmp/spark-events/{TODAY}",
    "spark:spark.eventLog.dir": f"gs://{SPARK_EVENTS_BUCKET}/tmp/spark-events/{TODAY}",
    "spark-env:SL_FS": "gs://${datastore_bucket}",
    "spark-env:SL_HIVE": "false",
    "spark-env:SL_GROUPED": COMET_GROUPED,
    #"spark-env:COMET_SINK_TO_FILE": COMET_SINK_TO_FILE,
    #"spark-env:COMET_SINK_AUDIT_TO_FILE": COMET_SINK_AUDIT_TO_FILE,
	#"spark-env:COMET_INTERMEDIATE_BQ_FORMAT": "orc", # Use for nullable data only
    "spark-env:SL_ROOT": "/${root_dir}",
    "spark-env:SL_AUDIT_SINK_TYPE": "BigQuerySink",
    "spark-env:SL_SINK_REPLAY_TO_FILE": "false", # replay file generation causes serious performance decrease
    "spark-env:SL_MERGE_OPTIMIZE_PARTITION_WRITE": "true",
    "spark-env:SL_SPARK_SQL_SOURCES_PARTITION_OVERWRITE_MODE": "dynamic",
    "spark-env:OUTGOING_FS": "gs://{outgoing_bucket}"
}

def generate_cluster_name(dag_name: str=DAG_NAME):
    d_name = "${dataproc_name}"
    dt = TODAY
    return f"{d_name}-{dag_name}-{dt}"

def create_dataproc_cluster(cluster_name: str,
                            project_id: str=PROJECT_ID,
                            subnet: str=SUBNET,
                            service_account: str=DATAPROC_SERVICE_ACCOUNT,
                            software_properties: dict=DEFAULT_CLUSTER_PROPERTIES,
                            task_id: str="create_dataproc_cluster",
                            trigger_rule=TriggerRule.ALL_SUCCESS,
                            master_conf: dict=DEFAULT_MASTER_CONF,
                            worker_conf: dict=DEFAULT_WORKER_CONF,
                            secondary_worker_conf: dict=None,
                            is_single_node: bool=False,
                            pool: str=DEFAULT_POOL,
                            **kwargs):
    """
    Create the Cloud Dataproc cluster.
    This operator will be flagged a success if the cluster by this name already exists.

    :param dag:
    :param secondary_worker_conf:
    EXAMPLE :
        {
            "num_instances": 2,
            "machine_type_uri": WORKER_MACHINE_TYPE,
            "disk_config": {"boot_disk_type": "pd-standard", "boot_disk_size_gb": 2048},
            "is_preemptible": True,
            "preemptibility": "PREEMPTIBLE"
        }
    :param worker_conf:
    EXAMPLE :
        {
            "num_instances": 4,
            "machine_type_uri": WORKER_MACHINE_TYPE,
            "disk_config": {"boot_disk_type": "pd-standard", "boot_disk_size_gb": 2048},
        }
    :param master_conf:
    EXAMPLE:
        {
            "num_instances": 1,
            "machine_type_uri": MASTER_MACHINE_TYPE,
            "disk_config": {"boot_disk_type": "pd-standard", "boot_disk_size_gb": 2048},
        }
    :param subnet: The subnetwork that will be used by the dataproc cluster
    :param software_properties:
    EXAMPLE :
            {"spark-env:SL_FS": "gs://{datastore_bucket}",
            "spark-env:SL_HIVE": "false",
            "spark-env:SL_GROUPED": "false",
            "spark-env:SL_ROOT": "/{root_dir}",
            "spark-env:OUTGOING_FS": "gs://{outgoing_bucket}",
            "spark-env:SL_UDFS": "com.natixis.gi6.external.MonetiqueUdf",
            "spark-env:SL_AUDIT_SINK_TYPE": "BigQuerySink",
            "spark-env:SL_LOAD_STRATEGY": "ai.starlake.job.load.IngestionNameStrategy",
            "spark-env:SL_ENV": "{env}",
            "dataproc:dataproc.logging.stackdriver.job.driver.enable": "true",
            "dataproc:dataproc.logging.stackdriver.job.yarn.container.enable": "true",
            "dataproc:job.history.to-gcs.enabled": "true",
            "dataproc:jobs.file-backed-output.enable": "true",
            "spark:spark.history.fs.logDirectory": "gs://dataproc-{project_id}/tmp/spark-events/{}".format(today),
            "spark:spark.eventLog.dir": "gs://dataproc-{project_id}/tmp/spark-events/{}".format(today),
            "spark:spark.eventLog.enabled": "true"}
    :param cluster_name:
    :param project_id:
    :param service_account: The path to the service account used by the dataproc cluster jobs
    :param task_id:
    :param trigger_rule:
    :param is_single_node: set to true to create a single node cluster
    :return:
    """

    if worker_conf is None:
        worker_conf = DEFAULT_WORKER_CONF
    if master_conf is None:
        master_conf = DEFAULT_MASTER_CONF
    if software_properties is None:
        software_properties = {}

    config = {
        "master_config": master_conf,
        "worker_config": worker_conf,
        "config_bucket": f"dataproc-{project_id}",
        "gce_cluster_config": {
            "service_account": service_account,
            "subnetwork_uri": f"projects/{project_id}/regions/{REGION}/subnetworks/{subnet}",
            "internal_ip_only": True,
            "tags": ["dataproc"]
        },
        "software_config": {
            "image_version": IMAGE_VERSION,
            "properties": {
                **software_properties
            }
        },
        "lifecycle_config": {
            "idle_delete_ttl": {"seconds": 3600}
        }
    }

    if is_single_node:
        config.pop("worker_config")
        config["software_config"]["properties"]["dataproc:dataproc.allow.zero.workers"] = "true"
    elif secondary_worker_conf is not None:
        config["secondary_worker_config"] = secondary_worker_conf

    return DataprocCreateClusterOperator(
        task_id=task_id,
        cluster_name=cluster_name,
        cluster_config=config,
        zone=ZONE,
        region=REGION,
        trigger_rule=trigger_rule,
        pool=pool,
        **kwargs
    )


def delete_dataproc_cluster(cluster_name: str,
                            task_id: str="delete_dataproc_cluster",
                            trigger_rule=TriggerRule.NONE_SKIPPED,
                            pool: str=DEFAULT_POOL,
                            **kwargs):
    """
    Tears down the cluster even if there are failures in upstream tasks.
    :param dag:
    :param cluster_name:
    :param task_id:
    :param trigger_rule:
    :return:
    """
    return DataprocDeleteClusterOperator(
        task_id=task_id,
        cluster_name=cluster_name,
        region=REGION,
        trigger_rule=trigger_rule,
        pool=pool,
        **kwargs
    )


def submit_starlake_job(cluster_name :str,
                  project_id :str=PROJECT_ID,
                  task_id :str=None,
                  arguments: list=None,
                  jar_list: list=JARS_LIST,
                  spark_properties: dict=DEFAULT_SPARK_PROPERTIES,
                  spark_config: dict=None,
                  main_class=MAIN_CLASS,
                  retries: int=0,
                  trigger_rule=TriggerRule.ALL_SUCCESS,
                  pool: str=DEFAULT_POOL,
                  **kwargs):
    """
    Create a dataproc job on the specified cluster

    :param dag:
    :param jar_list:
    :param trigger_rule:
    :param retries:
    :param main_class:
    :param task_id:
    :param project_id:
    :param cluster_name:
    :param arguments:
    :type spark_properties: object
    """
    if spark_properties is None:
        spark_properties = {}

    if spark_config is not None:
        spark_config_dict=ast.literal_eval(spark_config)
        mem_alloc=spark_config_dict['memAlloc']
        num_vcpu=spark_config_dict['numVcpu']
        spark_executor_instances=spark_config_dict['sparkExecutorInstances']
        spark_properties = spark_properties.copy()
        spark_properties.update({'spark.executor.memory': mem_alloc})
        spark_properties.update({'spark.executor.cores': num_vcpu})
        spark_properties.update({'spark.executor.instances': spark_executor_instances})

    spark_job = {
        "reference": {
            "project_id": project_id,
            "job_id": task_id + "_{{ execution_date.strftime('%Y-%m-%d-%H%M') }}_" + str(uuid.uuid4())[:8]
        },
        "placement": {"cluster_name": cluster_name},
        "spark_job": {
            "jar_file_uris": jar_list,
            "main_class": main_class,
            "args": arguments,
            "properties": spark_properties
        }
    }
    return DataprocSubmitJobOperator(
        task_id=task_id,
        job=spark_job,
        region=REGION,
        retries=retries,
        trigger_rule=trigger_rule,
        pool=pool,
        **kwargs
    )

def importJob(cluster_name :str,
                  project_id :str=PROJECT_ID,
                  task_id :str=None,
                  domain: str=None,
                  spark_properties: dict=DEFAULT_SPARK_PROPERTIES,
                  spark_config: dict=None,
                  retries: int=0,
                  trigger_rule=TriggerRule.ALL_SUCCESS,
                  pool: str=DEFAULT_POOL,
                  **kwargs):
    arguments = ["import", "--include", domain]
    return submit_starlake_job(
        cluster_name=cluster_name, 
        project_id=project_id, 
        task_id=task_id, 
        arguments=arguments, 
        spark_properties=spark_properties, 
        spark_config=spark_config, 
        retries=retries, 
        trigger_rule=trigger_rule, 
        pool=pool, 
        **kwargs
    )

def loadJob(cluster_name :str,
                  project_id :str=PROJECT_ID,
                  task_id :str=None,
                  domain: str=None,
                  table: str=None,
                  spark_properties: dict=DEFAULT_SPARK_PROPERTIES,
                  spark_config: dict=None,
                  retries: int=0,
                  trigger_rule=TriggerRule.ALL_SUCCESS,
                  pool: str=DEFAULT_POOL,
                  **kwargs):
    arguments = ["load", "--include", domain, "--schemas", table]
    return submit_starlake_job(
        cluster_name=cluster_name, 
        project_id=project_id, 
        task_id=task_id, 
        arguments=arguments, 
        spark_properties=spark_properties, 
        spark_config=spark_config, 
        retries=retries, 
        trigger_rule=trigger_rule, 
        pool=pool, 
        **kwargs
    )

def transformJob(cluster_name :str,
                  project_id :str=PROJECT_ID,
                  task_id :str=None,
                  task_name: str=None,
                  job_options: str=None,
                  spark_properties: dict=DEFAULT_SPARK_PROPERTIES,
                  spark_config: dict=None,
                  retries: int=0,
                  trigger_rule=TriggerRule.ALL_SUCCESS,
                  pool: str=DEFAULT_POOL,
                  **kwargs):
    arguments = ["transform", "--name", task_name]
    if job_options is not None:
        arguments.append("--options")
        arguments.append(job_options)
    return submit_starlake_job(
        cluster_name=cluster_name, 
        project_id=project_id, 
        task_id=task_id, 
        arguments=arguments, 
        spark_properties=spark_properties, 
        spark_config=spark_config, 
        retries=retries, 
        trigger_rule=trigger_rule, 
        pool=pool, 
        **kwargs
    )

def retrieve_query(dag_name_config: str, task_name_config: str, default: str) -> str:
    try:
        dag_config = Variable.get(dag_name_config, deserialize_json=True)
        conf_dt_plan = dag_config[task_name_config]
        if str(conf_dt_plan) != "":
            query_result = str(conf_dt_plan)
        else:
            query_result = default
    except:
        query_result = default
    return query_result


def retrieve_option(dag_name_config, task_name_config, option_name_config, default):
    try:
        dag_config = Variable.get(dag_name_config, deserialize_json=True)
        task_config = dag_config[task_name_config]
        option_config = task_config[option_name_config]
        if str(option_config) != "":
            option_result = str(option_config)
        else:
            option_result = default
    except:
        option_result = default
    return option_result