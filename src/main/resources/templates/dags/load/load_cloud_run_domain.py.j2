# This template executes individual bash jobs and requires the following dag generation options set:
#
# - SL_ROOT: The root project path
# - SL_STARLAKE_PATH: the path to the starlake executable
#

import os
from datetime import timedelta

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
from airflow.operators.python import BranchPythonOperator

from airflow.decorators import task
from airflow.exceptions import AirflowException
from airflow.utils.task_group import TaskGroup
from airflow.utils.trigger_rule import TriggerRule

#from gi6.cloud_run.operators import *
#from gi6.utils.functions import *

{% include 'dags/templates/cloud_run_operators.py.j2' %}

schedules= [{% for schedule in context.schedules %}
    {
        'schedule': '{{ schedule.schedule }}',
        'cron': {% if schedule.cron is not none %}'{{ schedule.cron }}'{% else %}None{% endif %},
        'domains': [{% for domain in schedule.domains %}
            {
                'name':'{{ domain.name }}',
                'final_name':'{{ domain.final_name}}',
                'tables': [{% for table in domain.tables %}
                    {
                        'name': '{{ table.name }}',
                        'final_name': '{{ table.final_name }}'
                    }{% if not loop.last  %},{% endif %}{% endfor %}
                ]
            }{% if not loop.last  %},{% endif %}{% endfor %}
        ]
    }{% if not loop.last  %},{% endif %}{% endfor %}
]

def generate_dag_name(schedule):
    base_dag_name = os.path.basename(__file__).replace(".py", "").replace(".pyc", "")
    return (f"{base_dag_name}-{schedule['schedule']}" if len(schedules) > 1 else base_dag_name)

@task(trigger_rule=TriggerRule.ONE_FAILED, retries=0, pool=DEFAULT_POOL)
def watcher():
    raise AirflowException("Failing task because one or more upstream tasks failed.")

# [START instantiate_dag]
for schedule in schedules:
    tags = get_context_var(var_name='tags', default_value="", options=options).split()
    for domain in schedule["domains"]:
        tags.append(domain["name"])
    with DAG(generate_dag_name(schedule),
             schedule_interval=schedule['cron'],
             default_args=DEFAULT_DAG_ARGS,
             catchup=False,
             tags=set([tag.upper() for tag in tags]),
             description=description) as dag:
        start = dummy_op(task_id="start")

        incoming_path = get_context_var(var_name='incoming_path', default_value=f'{SL_ROOT}/incoming', options=options)

        def generate_task_group_for_domain(domain):
            list_files = BashOperator(
                task_id=sanitize_id(f'{domain["name"]}_list_files'),
                bash_command=f'gsutil ls {incoming_path}/{domain["name"]}/* | wc -l',
                do_xcom_push=True,
#                pool=GNE_DEFAULT_POOL
            )

            start_import = dummy_op(task_id=sanitize_id(f'{domain["name"]}_start_import'), trigger_rule=TriggerRule.ONE_SUCCESS)

            start_load = dummy_op(task_id=sanitize_id(f'{domain["name"]}_start_load'), trigger_rule=TriggerRule.ALL_SUCCESS)

            skip_load = dummy_op(task_id=sanitize_id(f'{domain["name"]}_skip_load'), trigger_rule=TriggerRule.ONE_SUCCESS)

#            skip_load >> watcher

            def f_skip_or_start(**kwargs):
                task_instance = kwargs['ti']
                files_tuple = task_instance.xcom_pull(key=None, task_ids=[list_files.task_id])
                print('Number of files found: {}'.format(files_tuple))
                files_number = files_tuple[0]
                if int(files_number) > 1:
                    return start_import.task_id
                else:
                    return skip_load.task_id

            skip_or_start = BranchPythonOperator(
                task_id = sanitize_id(f'{domain["name"]}_skip_or_start'),
                python_callable = f_skip_or_start,
                trigger_rule = 'one_success',
                provide_context = True,
#                pool=GNE_DEFAULT_POOL
            )

            start >> list_files >> skip_or_start >> [start_import, skip_load]

            with TaskGroup(group_id=sanitize_id(f'{domain["name"]}_import_tasks')) as domain_import_tasks:
                import_task_id = sanitize_id(f'{domain["name"]}_import')
                import_task = importJob(
                    task_id=import_task_id, 
                    project_id=project_id, 
                    cloud_run_job_name=cloud_run_job_name,
                    cloud_run_job_region=cloud_run_job_region, 
                    domain=domain["name"],
                    sl_env_vars=SL_ENV_VARS
                )
                task_completion_sensors = jobCompletionSensors(
                    source_task_id=import_task_id, 
                    project_id=project_id, 
                    cloud_run_job_region=cloud_run_job_region, 
                    outlets=f'{project_id}.{domain["final_name"]}',
                    retry_exit_code=1
                )
                start_import >> import_task >> task_completion_sensors

            domain_import_tasks >> start_load

            with TaskGroup(group_id=sanitize_id(f'{domain["name"]}_load_tasks')) as domain_load_tasks:
                for table in domain["tables"]:
                    load_task_id = sanitize_id(f'{domain["name"]}_{table["name"]}_load')
                    load_task = loadJob(
                        task_id=load_task_id, 
                        project_id=project_id, 
                        cloud_run_job_name=cloud_run_job_name,
                        cloud_run_job_region=cloud_run_job_region, 
                        domain=domain["name"], 
                        table=table["name"],
                        sl_env_vars=SL_ENV_VARS
                    )
                    task_completion_sensors = jobCompletionSensors(
                        source_task_id=load_task_id, 
                        project_id=project_id, 
                        cloud_run_job_region=cloud_run_job_region, 
                        outlets=f'{project_id}.{domain["final_name"]}.{table["name"]}',
                        retry_exit_code=1
                    )
                    start_load >> load_task >> task_completion_sensors
            return domain_load_tasks

        all_load_tasks = [generate_task_group_for_domain(domain) for domain in schedule["domains"]]

        all_load_tasks >> watcher()
