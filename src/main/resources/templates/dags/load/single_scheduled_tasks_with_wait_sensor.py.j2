# This template executes individual cloud run jobs and requires the following dag generation options set:
#
# - cloud_run_job_name: the name of the job to execute
# - sl_env_var: the airflow variable name where a map in json is specified in order to add them as env var in cloud run job
# - global_ack_file_path: the path to the ack file to wait for
# - ack_wait_timeout: the time to wait in seconds
# - schedule: the schedule airflow variable where value is a cron
#
{% include 'templates/dags/__starlake_cloud_run_operator.py.j2' %}

import os

from airflow import DAG
from airflow.decorators import task
from airflow.exceptions import AirflowException
from airflow.providers.google.cloud.operators.gcs import \
    GCSDeleteObjectsOperator
from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor
from airflow.utils import trigger_rule
from airflow.utils.task_group import TaskGroup

schedules= [{% for schedule in context.schedules %}
    {
        'schedule': '{{ schedule.schedule }}',
        'cron': {% if schedule.cron is not none %}'{{ schedule.cron }}'{% else %}None{% endif %},
        'domains': [{% for domain in schedule.domains %}
            {
                'name':'{{ domain.name }}',
                'final_name':'{{ domain.final_name}}',
                'tables': [{% for table in domain.tables %}
                    {
                        'name': '{{ table.name }}',
                        'final_name': '{{ table.final_name }}'
                    }{% if not loop.last  %},{% endif %}{% endfor %}
                ]
            }{% if not loop.last  %},{% endif %}{% endfor %}
        ]
    }{% if not loop.last  %},{% endif %}{% endfor %}
]

@task(trigger_rule=trigger_rule.TriggerRule.ONE_FAILED, retries=0)
def watcher():
    raise AirflowException("Failing task because one or more upstream tasks failed.")

def generate_dag_name(schedule):
    base_dag_name = os.path.basename(__file__).replace(".py", "").replace(".pyc", "")
    return (f"{base_dag_name}-{schedule['cron']}" if len(schedules) > 1 else base_dag_name)

sl_operator = StarlakeCloudRunOperator()

for schedule in schedules:
    with DAG(generate_dag_name(schedule),
             schedule_interval=schedule["cron"],
             default_args=DEFAULT_DAG_ARGS,
             catchup=False,
             description=description) as dag:
        ack_wait_timeout = int(options["ack_wait_timeout"])
        ack_file=options["global_ack_file_path"]
        wait_for_ack = GCSObjectExistenceSensor(
            task_id="wait_for_ack",
            bucket="{{'{{var.json.BUCKET_INCOMING.name}}'}}",
            object=ack_file,
            timeout=ack_wait_timeout,
            mode="reschedule"
        )
        delete_ack = GCSDeleteObjectsOperator(
            task_id="delete_ack",
            bucket_name="{{'{{var.json.BUCKET_INCOMING.name}}'}}",
            objects=[ack_file]
        )

        def generate_task_group_for_domain(domain):
            with TaskGroup(group_id=sanitize_id(f'{domain["name"]}_load_tasks')) as domain_load_tasks:
                for table in domain["tables"]:
                    load_task_id = sanitize_id(f'{domain["name"]}_{table["name"]}')
                    load_task = sl_operator.sl_load(
                        task_id=load_task_id, 
                        domain=domain["name"], 
                        table=table["name"]
                    )
            return domain_load_tasks

        all_load_tasks = [generate_task_group_for_domain(domain) for domain in schedule["domains"]]

        wait_for_ack >> delete_ack >> all_load_tasks >> watcher()
