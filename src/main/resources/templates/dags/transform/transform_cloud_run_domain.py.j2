# This template executes individual bash jobs and requires the following dag generation options set:
#
# - SL_ROOT: The root project path
# - SL_STARLAKE_PATH: the path to the starlake executable
#

import os
from datetime import timedelta
import json

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

from airflow.utils.task_group import TaskGroup

#from gi6.utils.functions import *
#from gi6.cloud_run.operators import *

{% include 'dags/templates/cloud_run_operators.py.j2' %}
#cron="{{ context.cron }}"

task_deps=json.loads("""{{ context.dependencies }}""")

load_dependencies = get_context_var(var_name='load_dependencies', default_value='False', options=options)

def generate_dag_name():
    base_dag_name = os.path.basename(__file__).replace(".py", "").replace(".pyc", "")
    return base_dag_name

def create_task(airflow_task_id: str, transform_name: str, task_type: str):
    if (task_type == 'task'):
        transform_task = transformJob(
          task_id=airflow_task_id, 
          project_id=project_id, 
          cloud_run_job_name=cloud_run_job_name, 
          cloud_run_job_region=cloud_run_job_region, 
          task_name=transform_name, 
          sl_env_vars=SL_ENV_VARS
        )
        task_completion_sensors = jobCompletionSensors(
            source_task_id=airflow_task_id, 
            project_id=project_id, 
            cloud_run_job_region=cloud_run_job_region, 
            outlets=f'{project_id}.{transform_name}',
            retry_exit_code=1
        )
        transform_task >> task_completion_sensors
        return [transform_task, task_completion_sensors]
    else:
        load_domain_and_table = transform_name.split(".",1)
        domain = load_domain_and_table[0]
        table = load_domain_and_table[1]
        load_task = loadJob(
            task_id=airflow_task_id, 
            project_id=project_id, 
            cloud_run_job_name=cloud_run_job_name,
            cloud_run_job_region=cloud_run_job_region, 
            domain=domain, 
            table=table,
            sl_env_vars=SL_ENV_VARS
        )
        task_completion_sensors = jobCompletionSensors(
            source_task_id=airflow_task_id, 
            project_id=project_id, 
            cloud_run_job_region=cloud_run_job_region, 
            outlets=f'{project_id}.{domain}.{table}',
            retry_exit_code=1
        )
        load_task >> task_completion_sensors
        return [load_task, task_completion_sensors]

tags = get_context_var(var_name='tags', default_value="", options=options).split()

# [START instantiate_dag]
with DAG(generate_dag_name(),
         schedule_interval=None, #=cron FIXME
         default_args=DEFAULT_DAG_ARGS,
         catchup=False,
         tags=set([tag.upper() for tag in tags]),
         description=description) as dag:

    start = dummy_op(task_id="start")

    end = dummy_op(task_id='end')

    # build takgroups recursively
    def generate_task_group_for_task(transform_task):
        transform_name = transform_task['data']['name']
        airflow_task_group_id = sanitize_id(transform_name)
        airflow_task_id = airflow_task_group_id
        task_type = transform_task['data']['typ']
        if (task_type == 'task'):
            airflow_task_id = airflow_task_group_id + "_task"
        else:
            airflow_task_id = airflow_task_group_id + "_table"

        if (load_dependencies.lower() == 'true' and 'children' in transform_task):
            with TaskGroup(group_id=airflow_task_group_id) as airflow_task_group:
                for transform_sub_task in transform_task['children']:
                    generate_task_group_for_task(transform_sub_task)
                upstream_tasks = list(airflow_task_group.children.values())
                airflow_task = create_task(airflow_task_id, transform_name, task_type)
                airflow_task.set_upstream(upstream_tasks)
            return airflow_task_group
        else:
            airflow_task_group = create_task(airflow_task_id=airflow_task_id, transform_name=transform_name, task_type=task_type)
            airflow_task_group[-1] >> end
            return airflow_task_group[0]

    all_transform_tasks = [generate_task_group_for_task(task) for task in task_deps]

    start >> all_transform_tasks
