from datetime import timedelta, datetime
from airflow.decorators import task
from airflow.exceptions import AirflowException
from airflow.models import DAG, Variable
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator, DataprocCreateClusterOperator, DataprocDeleteClusterOperator
from airflow.providers.google.cloud.sensors.dataproc import DataprocJobSensor
from airflow.utils import trigger_rule
from airflow.utils.task_group import TaskGroup
import os
import re


dag_name = os.path.basename(__file__).replace(".py", "").replace(".pyc", "")
cluster_name = f"{dag_name}-{{"{{{{ data_interval_end | default(dag_run.logical_date) | ts_nodash | lower }}}}"}}"

DEFAULT_DAG_ARGS = {
    'depends_on_past': False,
    'start_date': datetime(1900, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1)
}

CLUSTER_CONFIG = Variable.get("{{sl.config().dataproc().get().profileVar().get()}}", deserialize_json=True)
CLUSTER_CONFIG["software_config"] = CLUSTER_CONFIG.get("software_config") or {"properties": {}}
CLUSTER_CONFIG["software_config"]["properties"] = CLUSTER_CONFIG["software_config"].get("properties") or {}

for key, value in Variable.get("{{sl.config().sl().get().envVar().get()}}", {}, deserialize_json=True).items():
    CLUSTER_CONFIG["software_config"]["properties"][f"spark-env:{key}"] = value

SPARK_DEFAULT_PROPERTIES = {
    'spark.sql.parquet.int96RebaseModeInWrite': 'CORRECTED',
    'spark.sql.catalogImplementation': 'in-memory',
    'spark.sql.adaptive.skewJoin.enabled': 'true',
    'spark.sql.cbo.enabled': 'true',
    'spark.sql.adaptive.enabled': 'true',
    'spark.sql.adaptive.coalescePartitions.enabled': 'true',
    'spark.sql.adaptive.coalescePartitions.initialPartitionNum': '8000',
    'spark.sql.adaptive.coalescePartitions.minPartitionSize': '1',
    'spark.sql.adaptive.localShuffleReader.enabled': 'true'
}

DATAPROC_JAR_FILE_URIS = Variable.get("{{sl.config().sl().get().jarFileUrisVar().get()}}", deserialize_json=True)

def submitJobFor(domain:str, table: str):
    return DataprocSubmitJobOperator(
        task_id=re.sub("[^a-zA-Z0-9\-_]", "_", f'{domain}_{table}'),
        region="{{sl.config().dataproc().get().region().get()}}",
        job={
            "placement": {"cluster_name": cluster_name},
            "spark_job": {
                "jar_file_uris": DATAPROC_JAR_FILE_URIS,
                "main_class": "ai.starlake.job.Main",
                "args": ['watch', '--include', domain, '--schemas', table],
                "properties": SPARK_DEFAULT_PROPERTIES
            },
            "labels": {"task":"watch","table": re.sub("[^a-zA-Z0-9\-_]", "_", f'{table}_{domain}')[0:63].lower()}
        },
        retries=0,
        asynchronous=True
    )

def waitForDataprocJob(dataproc_job):
    return DataprocJobSensor(
        task_id=dataproc_job.task_id[dataproc_job.task_id.index('.')+1:] + '_sensor',
        region="{{sl.config().dataproc().get().region().get()}}",
        dataproc_job_id=dataproc_job.output,
        poke_interval=10,
        mode="reschedule"
    )

@task(trigger_rule=trigger_rule.TriggerRule.ONE_FAILED, retries=0)
def watcher():
    raise AirflowException("Failing task because one or more upstream tasks failed.")

with DAG(dag_name, schedule="{{sl.config().schedule().get()}}" or None, default_args=DEFAULT_DAG_ARGS, catchup = False) as dag:

    create_cluster = DataprocCreateClusterOperator(
        task_id='create_dataproc_cluster',
        cluster_name=cluster_name,
        region="{{sl.config().dataproc().get().region().get()}}",
        cluster_config=CLUSTER_CONFIG
    )

    with TaskGroup(group_id='load_tasks') as load_tasks:
        load_task_dict = {}
        sensor_task_dict = {}
{% for domainTable in sl.domainTables() %}
        load_task_dict['{{domainTable.domain().name()}}_{{domainTable.table().name()}}'] = submitJobFor('{{domainTable.domain().name()}}', '{{domainTable.table().name()}}')
        sensor_task_dict['{{domainTable.domain().name()}}_{{domainTable.table().name()}}'] = waitForDataprocJob(load_task_dict['{{domainTable.domain().name()}}_{{domainTable.table().name()}}'])
        load_task_dict['{{domainTable.domain().name()}}_{{domainTable.table().name()}}'] >> sensor_task_dict['{{domainTable.domain().name()}}_{{domainTable.table().name()}}']
{% endfor %}
    # Delete the Cloud Dataproc cluster.
    delete_cluster = DataprocDeleteClusterOperator(
        task_id='delete_dataproc_cluster',
        region="{{sl.config().dataproc().get().region().get()}}",
        cluster_name=cluster_name,
        trigger_rule=trigger_rule.TriggerRule.ALL_DONE
    )

    create_cluster >> load_tasks >> [delete_cluster, watcher()]