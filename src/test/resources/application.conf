include required("reference.conf")

# NOTE: this file should contain deviations from the standard ("production default") reference.conf for use
# in the Test context
# Typelevel Config loads application.conf first and if it doesn't find it, reference.conf

extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker

archive = true

file-system = "file://"

hive = false

grouped = true

analyze = false

jdbc = {
  "test-h2": {
#    uri = "jdbc:h2:mem:testNameFIXME", # once #95 is solved we'll have one 'database' per instance
    uri = "jdbc:h2:/tmp/h2-testNameFIXME", # once #95 is solved we'll have one 'database' per instance
    driver = "org.h2.Driver"
    user="sa"
    password="sa"
  }
}

audit {
  index = "JDBC"
  options = {
    "bq-dataset": "audit",
    "jdbc": "test-h2",
    "jdbc-audit-table": """CREATE TABLE IF NOT EXISTS AUDIT (
                              jobid VARCHAR(255) not NULL,
                              paths VARCHAR(255) not NULL,
                              domain VARCHAR(255) not NULL,
                              schema VARCHAR(255) not NULL,
                              success BOOLEAN not NULL,
                              count INTEGER not NULL,
                              countOK INTEGER not NULL,
                              countKO INTEGER not NULL,
                              timestamp TIMESTAMP not NULL,
                              duration INTEGER not NULL,
                              message VARCHAR(255) not NULL
                             )
    """,
    "jdbc-rejected-table": """CREATE TABLE IF NOT EXISTS REJECTED (
                              jobid VARCHAR(255) not NULL,
                              timestamp TIMESTAMP not NULL,
                              domain VARCHAR(255) not NULL,
                              schema VARCHAR(255) not NULL,
                              error VARCHAR(255) not NULL,
                              path VARCHAR(255) not NULL
                             )
    """
  }

}

spark {
  debug.maxToStringFields=100
  master = "local[*]"
}
