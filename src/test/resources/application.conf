include required("reference.conf")

# NOTE: this file should contain deviations from the standard ("production default") reference.conf for use
# in the Test context
# Typelevel Config loads application.conf first and if it doesn't find it, reference.conf

extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker

archive = true

file-system = "file://"

hive = false

grouped = true

analyze = false

metrics {
  active = false
  active = ${?COMET_METRICS_ACTIVE}

  path = "/tmp/metrics/{domain}/{schema}"
  path = ${?COMET_METRICS_PATH}

  discrete-max-cardinality = 10
  discrete-max-cardinality = ${?COMET_METRICS_DISCRETE_MAX_CARDINALITY}

  infer = true
  infer = ${?COMET_METRICS_INFER}

  max-errors = 100
  index = "NONE"
  options = {
    "bq-dataset": "audit"
  }

}


audit {
  active = true
  active = ${?COMET_AUDIT_ACTIVE}

  #  path = "/tmp/metrics/{domain}/{schema}"
  path = "/tmp/audit"
  path = ${?COMET_AUDIT_PATH}

  audit-timeout = -1
  audit-timeout = ${?COMET_LOCK_AUDIT_TIMEOUT}

  max-errors = 100
  index = "NONE"
  options = {
    "bq-dataset": "audit",
    "jdbc-uri": "",
    "jdbc-user": "",
    "jdbc-password": ""
  }

}

spark {
  debug.maxToStringFields=100
  master = "local[*]"
}
