extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker

datasets = "/tmp/datasets"
datasets = ${?COMET_DATASETS}

tmpdir = "/tmp/comet_tmp"
tmpdir = ${?COMET_TMPDIR}


metadata = "/tmp/metadata"
metadata = ${?COMET_METADATA}

launcher = simple
launcher = ${?COMET_LAUNCHER}

archive = true
archive = ${?COMET_ARCHIVE}


hive = false
hive = ${?COMET_HIVE}

analyze = false
analyze = ${?COMET_ANALYZE}

airflow {
  endpoint = "http://127.0.0.1:8080/api/experimental"
  endpoint = ${?AIRFLOW_ENDPOINT}
}

writeFormat = parquet
writeFormat = ${?COMET_WRITE_FORMAT}

stat {
  discrete_max_cardinality = 10
  discrete_max_cardinality = ${?COMET_DISCRETE_MAX_CARDINALITY}
}

area {
  pending = "pending"
  pending = ${?COMET_PENDING}
  unresolved = "unresolved"
  unresolved = ${?COMET_UNRESOLVED}
  archive = "archive"
  archive = ${?COMET_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?COMET_INGESTING}
  accepted = "accepted"
  accepted = ${?COMET_ACCEPTED}
  rejected = "rejected"
  rejected = ${?COMET_REJECTED}
  business = "business"
  business = ${?COMET_BUSINESS}
}
spark {
  #  sql.hive.convertMetastoreParquet = false
  #  extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
  #  sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
  #  yarn.principal = "invalid"
  #  yarn.keytab = "invalid"
  #  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
  #  yarn.keytab = ${?SPARK_YARN_KEYTAB}
  master = "local[*]"
  #  sql.catalogImplementation="hive"
  #  sql.catalogImplementation="in-memory"
}
